---
title: "Puruki_ITS_DADA2"
author: "Alexa Byers"
date: "21/07/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
#if (!requireNamespace("BiocManager", quietly = TRUE))
 #   install.packages("BiocManager")
#BiocManager::install(version = '3.13', ask= FALSE)
#BiocManager::install("dada2", version = "3.13", force = TRUE)
#BiocManager::install("ShortRead")
#BiocManager::install("Biostrings")
library("dada2")
library("ShortRead")
library("Biostrings")
```
```{r}
path <- "~/AGRF_CAGRF21067226_JV8MM"
list.files(path)
```
#Forward and reverse fastq filenames have format: <samplenum>_JV8MM_<index>_L001_<readNum>.fastq.gz
```{r}
fnFs <- sort(list.files(path, pattern="_L001_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_L001_R2.fastq.gz", full.names = TRUE))
#Extract sample names
#fnFs
sample.names <- sapply(strsplit(basename(fnFs), "_"), '[', 1)
```
```{r}
#----- check read quality-----
plotQualityProfile(fnFs[1:4])
plotQualityProfile(fnRs[1:4])
```
#primer checks
```{r}
FWD <- "CTTGGTCATTTAGAGGAAGTAA"  
REV <- "GCTGCGTTCTTCATCGATGC"

allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
```
#pre-filter sequences to remove those with Ns but perform no other filtering
```{r}
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = FALSE)
```
#count number of times primer appears in forward and reverse reads
```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
```
##remove primers using cutadapt (cite: DOI:10.14806/ej.17.1.200)
```{r}
cutadapt <- "~/cutadapt" # CHANGE ME to the cutadapt path on your machine
system2(cutadapt, args = "--version") # Run shell commands from R
```
```{r}
path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
```
```{r}
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, "-o", fnFs.cut[i], "-p", fnRs.cut[i], fnFs.filtN[i], fnRs.filtN[i])) 
}
```
#count presence of primers in first cutadapt sample
```{r}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[5]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[5]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[5]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[5]]))
```
#primer free sequence files
```{r}
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_1.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_2.fastq.gz", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)
```

#----- check read quality-----
```{r}
plotQualityProfile(cutFs[1:4])
plotQualityProfile(cutFs[1:4])
```
# ------filter and trim-----
```{r}
newpath <-"~/cutadapt"
filtFs <- file.path(newpath, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(newpath, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

filtered_out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, maxN = 0, maxEE = c(2, 5), 
    truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = FALSE)
head(filtered_out)            

errF <- learnErrors(filtFs, multithread=FALSE)
errR <- learnErrors(filtRs, multithread=FALSE)
```
```{r}
# dereplicate reads
derep_filtFs <- derepFastq(filtFs[exists], verbose=TRUE)
names(derep_filtFs) <- sample.names[exists] # the sample names in these objects are initially the file names of the samples, this sets them to the sample names for the rest of the workflow
derep_filtRs <- derepFastq(filtRs[exists], verbose=TRUE)
names(derep_filtRs) <- sample.names[exists]

# inferring ASV
dadaFs <- dada(derep_filtFs, err=errF, multithread=FALSE)
dadaRs <- dada(derep_filtRs, err=errR, multithread=FALSE)
```
```{r}
# merging ASV
merged_amplicons <- mergePairs(dadaFs, derep_filtFs, dadaRs, derep_filtRs, verbose=TRUE)
seqtab <- makeSequenceTable(merged_amplicons)
reads.per.seqlen <- tapply(colSums(seqtab), factor(nchar(getSequences(seqtab))), sum)

# remove chimeric ASV
seqtab.nochim <- removeBimeraDenovo(seqtab, multithread=TRUE, verbose=TRUE)
```
```{r}
# final summary table
getN <- function(x) sum(getUniques(x))
summary_tab <- data.frame(row.names=sample.names[exists], dada2_input=filtered_out[exists,1],
               filtered=filtered_out[exists,2], dada_f=sapply(dadaFs, getN),
               dada_r=sapply(dadaRs, getN), merged=sapply(merged_amplicons, getN),
               nonchim=rowSums(seqtab.nochim),
               final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[exists,1]*100, 1))

# assign taxonomy
taxa <- assignTaxonomy(seqtab.nochim, "~/sh_general_release_dynamic_s_04.02.2020.fasta", multithread = TRUE, tryRC = TRUE)
# giving our seq headers more manageable names (ASV_1, ASV_2...)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

# writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs.fa")

# count table:
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

# tax table:
asv_tax <- taxa
row.names(asv_tax) <- sub(">", "", asv_headers)
write.table(asv_tax, "ASVs_taxonomy.tsv", sep="\t", quote=F, col.names=NA)
summary(cars)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

#filter and trim
#assign new file names for filtered files and place in new directory
```{r}
newpath <- "~/ITS DADA2"
filtFs <- file.path(newpath, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(newpath, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

filtered_out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, maxN = 0, maxEE = c(2, 5), 
    truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = FALSE)
head(filtered_out)
```
```{r}
errF <- learnErrors(filtFs, multithread=FALSE)
errR <- learnErrors(filtRs, multithread=FALSE)
```
#learn the error rates of thea DADA2 algorithm
```{r}
errF <- learnErrors(filtFs, multithread = FALSE)
errR <- learnErrors(filtRs, multithread = FALSE) #windows does not support multithread so set to FALSE
```
# dereplicate identical reads
```{r}
derep_filtFs <- derepFastq(filtFs[exists], verbose=TRUE)
names(derep_filtFs) <- sample.names[exists] # the sample names in these objects are initially the file names of the samples, this sets them to the sample names for the rest of the workflow
derep_filtRs <- derepFastq(filtRs[exists], verbose=TRUE)
names(derep_filtRs) <- sample.names[exists]
```
# inferring ASV
```{r}
dadaFs <- dada(derep_filtFs, err=errF, multithread=FALSE)
dadaRs <- dada(derep_filtRs, err=errR, multithread=FALSE)
```
# merging ASV
```{r}
merged_amplicons <- mergePairs(dadaFs, derep_filtFs, dadaRs, derep_filtRs, verbose=TRUE)
seqtab <- makeSequenceTable(merged_amplicons)
reads.per.seqlen <- tapply(colSums(seqtab), factor(nchar(getSequences(seqtab))), sum)
write.table(reads.per.seqlen, "ASV_length.txt")
write.table(seqtab, "merged.txt")
```
# remove chimeric ASV
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, multithread=FALSE, verbose=TRUE)
write.table(seqtab.nochim, "seqtab_nochim.txt")
```
# final summary table
```{r}
getN <- function(x) sum(getUniques(x))
summary_tab <- data.frame(row.names=sample.names[exists], dada2_input=filtered_out[exists,1],
               filtered=filtered_out[exists,2], dada_f=sapply(dadaFs, getN),
               dada_r=sapply(dadaRs, getN), merged=sapply(merged_amplicons, getN),
               nonchim=rowSums(seqtab.nochim),
               final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[exists,1]*100, 1))
```
# assign taxonomy
```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/sh_general_release_dynamic_10.05.2021.fasta", multithread = FALSE, tryRC = TRUE)
```
# giving our seq headers more manageable names (ASV_1, ASV_2...)
```{r}
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}
```
```{r}
# writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs.fa")

# count table:
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

# tax table:
asv_tax <- taxa
row.names(asv_tax) <- sub(">", "", asv_headers)
write.table(asv_tax, "ASVs_taxonomy.tsv", sep="\t", quote=F, col.names=NA)
summary(cars)
```


