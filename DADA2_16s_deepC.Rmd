---
title: "16s_data2"
author: "Alexa Byers"
date: "19/04/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
#install binaries from bioconductor
#once installed these steps won't need to be done again
if (!requireNamespace("BiocManager", quietly = TRUE))
 install.packages("BiocManager")
BiocManager::install(version = '3.13', ask= FALSE)
BiocManager::install("dada2", version = "3.13n")
library(dada2)
library(ShortRead)
```

```{r}
path <- "C:/Users/ByersA/OneDrive - scion/Soil deep C data/16s sequencing data/16S_AGRF_CAGRF21035708_JK2V2"
list.files(path)
```

```{r}
#forward and reverse fastq filenames have format : <samplenum>_JK2V2_<index>_L001_<readNum>.fastq.gz
fnFs <- sort(list.files(path, pattern = "_L001_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_L001_R2.fastq.gz", full.names = TRUE))
```

```{r}
#checking for primers
FWD <- "GTGYCAGCMGCCGCGGTAA" 
REV <-"CCGYCAATTYMTTTRAGTTT" 

allOrients <- function(primer) {
  #create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)
  orients <- c(Foward = dna, Complement = complement(dna), Reverse = reverse(dna),
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString)) #convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients

primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[103]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs[[103]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs[[103]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs[[103]]))
```

```{r}
#check read quality
plotQualityProfile(fnFs[1:4])
plotQualityProfile(fnRs[1:4])
```

```{r}
#filter and trim
#assign new file names for filtered files and place in new directory
newpath<- "C:/Users/ByersA/OneDrive - scion/Soil deep C data/16s_tax4fun2"
filtFs <- file.path(newpath, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(newpath, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
#filtFs
names(filtFs) <- sample.names
names(filtRs) <- sample.names
filtered_out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, maxEE = c(2,5), truncLen = c(240,220))
print(filtered_out)
#learn the error rates of thea DADA2 algorithm
errF <- learnErrors(filtFs, multithread = FALSE)
errR <- learnErrors(filtRs, multithread = FALSE) #windows does not support multithread so set to FALSE
```

```{r}
#dereplicate reads
derep_filtFs <- derepFastq(filtFs, verbose=TRUE)
names(derep_filtFs) <- sample.names #the sample names in these objects are initially the file names of the samples, this sets them to the sample names for the rest of the workflow
derep_filtRs <- derepFastq(filtRs, verbose=TRUE)
names(derep_filtRs) <- sample.names
```

```{r}
#inferring ASV
dadaFs <- dada(derep_filtFs, err=errF, multithread=FALSE)
dadaRs <- dada(derep_filtRs, err=errR, multithread=FALSE)
```

```{r}
#merging ASV
merged_amplicons <- mergePairs(dadaFs, derep_filtFs, dadaRs, derep_filtRs, trimOverhang=TRUE, verbose=TRUE)
seqtab <- makeSequenceTable(merged_amplicons)
```

```{r}
#remove chimeric ASV
seqtab.nochim <- removeBimeraDenovo(seqtab, multithread=FALSE, verbose=TRUE)
write.table(seqtab.nochim, "seqtab_nochim.txt")
#check what proportion of chimeras make up the dataset 
sum(seqtab.nochim)/sum(seqtab)
```

```{r}
#final summary table
getN <- function(x) sum(getUniques(x))
summary_tab <- data.frame(row.names=sample.names, dada2_input=filtered_out[,1],
                          filtered=filtered_out[,2], dada_f=sapply(dadaFs, getN),
                          dada_r=sapply(dadaRs, getN), merged=sapply(merged_amplicons, getN),
                          nonchim=rowSums(seqtab.nochim),
                          final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 1))
head(summary_tab)
write.table(summary_tab, "final_summary.txt")
```

```{r}
# assign taxonomy
#for assign taxonomy, 16s dataset was downloaded from https://benjjneb.github.io/dada2/training.html
taxa <- assignTaxonomy(seqtab.nochim, "C:/Users/ByersA/OneDrive - scion/Soil deep C data/16s sequencing data/16s_dada2/rdp_train_set_16.fa.gz", multithread=FALSE)


# giving our seq headers more manageable names (ASV_1, ASV_2...)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

# writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs.fa")

# count table:
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

# tax table:
asv_tax <- taxa
row.names(asv_tax) <- sub(">", "", asv_headers)
write.table(asv_tax, "ASVs_taxonomy.tsv", sep="\t", quote=F, col.names=NA)
```

